{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundmentals of Big Data -- 07 November 2019\n",
    "## Afternoon session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data Options\n",
    "[Spark][SPARK] most important part of [Hadoop][HADOOP]. \n",
    "\n",
    "### What's your Big Data Flavor? \n",
    "All open-source Apache options use [Hadoop][HADOOP] underneith, \n",
    "different companies sell different flavors cloudra, HortonWorks, MAPR, IBM InfoSphere,\n",
    "Amazon EMR, Microsoft HDInsight. All similar because they are based on Apache stack on Apache Hadoop. \n",
    "If you understand one, understand the other companies. 80% or more the same.\n",
    "\n",
    "### Big Data Cloud Offerings\n",
    "- Amazon EMR (Elastic Map Reduce) Web services\n",
    "- Google Dataproc \n",
    "- Microsoft Azure HDInsight\n",
    "\n",
    "Any Cloud provider IAAS (infrastructure as a service). Industry moving towards the cloud. HortonWorks recent merges, getting more competitation like Amazon and Microsoft. Some acquistions happening, Google is on a buying sphree. Merges, most of towards the cloud. Cloud option, don't haver to manage master and slave modes other than managing the servers. \n",
    "\n",
    "### Key Differentatiors\n",
    "\n",
    "#### Cloudera \n",
    "Develop their own tools as add-on. Merging with Horton Works. \n",
    "\n",
    "- Cloudera\n",
    "- Impala\n",
    "\n",
    "#### Horton Works\n",
    "Staying open source. Developing Open Source Tools e.g. Ambari\n",
    "\n",
    "#### MapR\n",
    "Proprietary implementaiton. Have MapRFS - High Availability\n",
    "\n",
    "#### Amazon EMR\n",
    "Cloud offering. Integration with other AWS Services\n",
    "\n",
    "### Spark Essentials\n",
    "Most important of the Hadoop ecoystem\n",
    "\n",
    "Challenges with MapReduce. HDFS<-Read-Map. I/O are expensive, move from disk to memory and consume processing power. Read/writes for the same data that you could process the data. Separate.\n",
    "- Also there is replication and serialization happendinf during the processing - making the processing even slower\n",
    "- As the amount of data processes\n",
    "\n",
    "Invention creation of Spark. Apache Spark is an open source cluster computing framework for Batch and Real Time processing. Deveoped by AMP-Lab (UC Berkley) in 2009, open-source in 2010. Apache foundation in 2013\n",
    "\n",
    "A unified analytics engine for large real-time processing\n",
    "\n",
    "Provides interface for programming entire clusters with data parallelism and fault-tolerance. \n",
    "\n",
    "\n",
    "[SPARK]: https://spark.apache.org/\n",
    "[HADOOP]: https://hadoop.apache.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After Spark - Ecosystem.\n",
    "\n",
    "Spark Core Engine replace YARN MapReduce v2.\n",
    "\n",
    "Spark RDD Resilient Distributive Dataset replaces Pig. HIVE replaces with Spark SQL. Spark R replaced R Combutors. Spark Mlib replaces Mahoub for machine learning. Spark Graph X (new), Spark Deep Learning, \n",
    "\n",
    "Flume replaced by Spark Streaming Data. On-the-fly analysis of the log streaming values. Very nicer approach for learning Spark and get all of the benefits of the Hadoop ecosystem. [Hadoop][HADOOP], one of storge, Spark replacing processing [Hadoop][HADOOP], still need storage. Prowerful just focusing on processing of data. Can cannot to most types of databases. [Spark][SPARK]. \n",
    "\n",
    "Starting any new project, start with [Spark][SPARK], start with Spark SQL engine move onto HDFS. Start with core engine and not MapReduce. \n",
    "Spark can use as standalone. \n",
    "\n",
    "[SPARK]: https://spark.apache.org/\n",
    "[HADOOP]: https://hadoop.apache.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databricks Demo\n",
    "Way cool!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Features\n",
    "\n",
    "**Speed** - [Spark][SPARK] Runs work loads 100 times faster than MapReduce\n",
    "\n",
    "**Ease of Use** - Write applications in Java, Python, Scala, R and SQL. Core Spark Core Engine are built with Scala, overall Python, R, and Java work really well in [Spark][SPARK]\n",
    "\n",
    "**Generality** - Combine SQL, streaming, and complex analytics\n",
    "\n",
    "**Runs Everywhere** - Runs on Hadoop, Apache Meso, Kubernets, Sandalone, Cloud\n",
    "\n",
    "**Real Time** - Use of in-memory computation helps low latency\n",
    "\n",
    "**Powerful Caching** - Both disk and memory caching\n",
    "\n",
    "\n",
    "## MapReduc v/s Spark\n",
    "\n",
    "Storge - MapReduce Disk Only, [Spark][SPARK] in memory or on disk\n",
    "\n",
    "Operation - Map and Rduce, [Spark][SPARK] Map, Reduce, Join, Sample, etc.\n",
    "\n",
    "Executions Model - Batch, [Spark][SPARK] batch, interactive, streaming\n",
    "\n",
    "Programming Environments - Java, [Spark][SPARK] - Scala, Java, Python, and R. Also can use SQL\n",
    "\n",
    "\n",
    "\n",
    "## Spark Context\n",
    "1.  Spark Context object in driver program is the entry point of spark application. It coordinates processes and resource allocations\n",
    "1.  Cluster managers provides access to executors (JVM processes)\n",
    "1.  Spark Context object sends application to executor\n",
    "1.  Spark Context eecutes task in each excutor\n",
    "\n",
    "\n",
    "Use `spark-submit` to run Python scripts using [pyspark](https://pypi.org/project/pyspark/)\n",
    "\n",
    "## Spark Components\n",
    "-  **Spark SQL** - Structured data processing. Can run queries from existing Hadoop Deployment. \n",
    "-  **Spark Streaming** - Enabled scalable, high througput stream processing of live data\n",
    "-  **MLib (Machine Learning)** - Machine learning libraries to run on top of [Spark][SPARK]\n",
    "-  **GraphX Processing** - Support of Graph and Graph parallel computing, Supports SPARQL\n",
    "-  **SparkR** - Support of R language\n",
    "\n",
    "## Spark Environment\n",
    "RDD API \n",
    "\n",
    "Data Frame - rows and columns of data\n",
    "\n",
    "[Sparks][SPARK] remembers, if you lose something it easy to replicated and restore.\n",
    "\n",
    "[SPARK]: https://spark.apache.org/\n",
    "[HADOOP]: https://hadoop.apache.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD to the rescue\n",
    "*  Want to reduce the I/0 operations\n",
    "*  Shared memory will be faster than network or disk sharing\n",
    "*  How to design such distributed memory sharing model that is fault tolerant and also efficient?\n",
    "\n",
    "RDD is the solution\n",
    "\n",
    "## Resilient Distrivuted Dataset (RDD)\n",
    "*  Primary abstraction in Spark\n",
    "*  Represents partitions across cluster nodes\n",
    "*  Enables parallel processing on data sets\n",
    "*  Partitions can be in-memroy or on-disk\n",
    "*  Immutable, re-computable and fault tolerant\n",
    "*  Track lineage information to efficiently re-compute lost data\n",
    "\n",
    "## RDD Features\n",
    "*  In-memory compution - in memory processing\n",
    "*  Lazy Evaluations - All tranformations are lazy, results are not computed right away\n",
    "*  Fault Toleerant - lineage tracking to rebuild lost data\n",
    "*  Immutable - Can be create or retrieved anytime\n",
    "*  Partitioning - fundemental\n",
    "*  Persistence - Can choos storage options -when and where\n",
    "\n",
    "## Ways to Create RDD\n",
    "\n",
    "-  Parallelized Containers \n",
    "-  From RDDs \n",
    "-  External Data, any other data structure\n",
    "\n",
    "## RDD Operations\n",
    "Two Types of operations on RDD. \n",
    "1.  Transformation creates a new RDD based on current RDD. \n",
    "1.  Action returns value\n",
    "\n",
    "## RDD Transformations\n",
    "*  Create new dataset from an existing one\n",
    "*  Use lazy evaluation: results not computed right away instead Spark remembers set of transformations applied to base dataset\n",
    "* These properties help in:\n",
    "  *  Optimizing the required calculations\n",
    "  *  Recovering lost data partitions and slow workers\n",
    "  \n",
    "## Examples\n",
    "\n",
    "### map\n",
    "Returns a new distributed dataset formed by passing each element of the source through function\n",
    "\n",
    "### filter\n",
    "Filters elements of dataset to a single \n",
    "\n",
    "### distinct\n",
    "Returns a new dataset formed by removing duplicates\n",
    "\n",
    "#### sortBy\n",
    "Sort returns a new dataset formated by removing duplicates\n",
    "\n",
    "### intersection\n",
    "Returns a new RDD that contains the intersection of elements in the source dataset and the argument\n",
    "\n",
    "### flatMap\n",
    "Similar to map, but each input item can mapped to 0 or more output items (so *func* should return a Seq rather than a single item)\n",
    "\n",
    "### union\n",
    "Returns a new dataset that contains the union of the elements in the source dataset\n",
    "\n",
    "### groupBy\n",
    "When called on a dataset of (K,V) pairs, returns a dataset of (K, iterable<V>) pairs\n",
    "    \n",
    "\n",
    "[SPARK]: https://spark.apache.org/\n",
    "[HADOOP]: https://hadoop.apache.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## map\n",
    "data = sc.parallelized(\"Hello Harry? How is your day?\".split(\" \"))\n",
    "modified_data = data.map(lambda work: (word, word[0], word.startswith('H')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Actions\n",
    "\n",
    "### reduce\n",
    "Aggregates the elements of the dataset using a function *func* (which takes two arguments and returns one). The function should be communicate and associateive so that can be computed correctly in parallel\n",
    "\n",
    "### first\n",
    "Returns the first element in RDD\n",
    "\n",
    "## count\n",
    "Number of elements in dataset\n",
    "\n",
    "## countByValue \n",
    "Only available on RDDs of type (K,V). Retuns a hash-mp of (K, Int) paris by values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy Evaluation\n",
    "*  Transformations are lazy in nature - when some operation is called on RDD, there is nothing executed - only a record of operation is maintained\n",
    "*  List of operations that where not executed get executed when an action is called\n",
    "\n",
    "## Lineage\n",
    "*  Each transformation creates a new child RDD\n",
    "*  Spark keeps track of parent RDD for each child RDD\n",
    "*  Action causes all transformations to run\n",
    "\n",
    "Transformations are not run until an action, RDDs are a depenancy chart until the actions are applied. \n",
    "\n",
    "## RDD Persistence\n",
    "*  Many times we might want to use the same RDD multiple imtes, which results in recomputing the RDD\n",
    "*  To avoid such scenario, persist the RDD (e.g. rdd1.persist())\n",
    "*  Node that computes the RDD, will store the partitions\n",
    "*  If Node fails, then Spark will recompute the lost partition\n",
    "*  Refer to section RDD persistence for different persistence options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
